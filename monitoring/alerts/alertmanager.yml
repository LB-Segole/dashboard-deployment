global:
  resolve_timeout: 5m
  slack_api_url: '${SLACK_ALERT_WEBHOOK}'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

templates:
  - '/etc/alertmanager/templates/*.tmpl'

route:
  receiver: 'slack-notifications'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h

  routes:
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 30s
      repeat_interval: 1h

    - match:
        severity: warning
      receiver: 'slack-notifications'
      group_wait: 1m
      repeat_interval: 2h

receivers:
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#voiceai-alerts'
        send_resolved: true
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        title_link: '{{ template "slack.link" . }}'
        footer: 'VoiceAI Monitoring'
        parse: 'full'

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_KEY}'
        send_resolved: true
        description: '{{ template "pagerduty.description" . }}'
        severity: '{{ if eq .Status "firing" }}critical{{ else }}info{{ end }}'
        class: '{{ .Labels.alertname }}'
        group: '{{ .Labels.service }}'
        component: '{{ .Labels.instance }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          num_firing: '{{ .Alerts.Firing | len }}'

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service'] 